p8105_hw5_mw3845
================
Minghe Wang
2024-11-14

# Problem 1

``` r
check_duplicate_bd <- function(n) {
  # Simulate uniformly distributed birthday with size n
  birthdays <- sample(1:365, n, replace = TRUE)
  # Check duplicate
  return(any(duplicated(birthdays)))
}
#Simulate each size 10000 times
sim_num = 10000
group_size = 2:50
prob = numeric(length(group_size))

for (i in seq_along(group_size)){
  n = group_size[i]
  results = replicate(sim_num, check_duplicate_bd(n))
  prob[i] = mean(results)
}

########################
#test
data <- data.frame(GroupSize = group_size, Probability = prob)

# Create the plot
ggplot(data, aes(x = GroupSize, y = Probability)) +
  geom_line(color = 'blue') +
  #geom_point(color = 'blue') +
  labs(title = 'Probability of Duplicate Birthday vs. Group Size',
       x = 'Group Size (n)',
       y = 'Probability of Duplicate Birthday') +
  theme_minimal()
```

![](p8105_hw5_mw3845_files/figure-gfm/problem1-1.png)<!-- -->

In this problem, we simulate a problem to measure the probability of
duplicate birthday among the group of `n` people. According to the plot,
we see a positive relationship between `group_size` and
`probability_of_duplicate_birthday`. The result aligns with our
intuition that with more people in a group, there are more likely to be
people with duplicate birthday.

\#Problem 2

``` r
#fixed parameters for simulation
n = 30                              #fixed sample size
sigma = 5                           #fixed sigma
ds_num = 5000                       #fixed number of datasets
alpha = 0.05

#Write function to get estimate and p-value for each dataset
sim_estimate_pval = function(mu, n = 30, sigma = 5){
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)
  )
  
  t.test(sim_data$x, mu = 0) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
}

sim_results_df = 
  expand_grid(
    mu_vec = c(0, 1, 2, 3, 4, 5, 6),
    iter = 1:ds_num
  ) |> 
  mutate(
    estimate_df = map(mu_vec, sim_estimate_pval)
  ) |> 
  unnest(estimate_df)

sim_results_df |> 
  pivot_longer(
    estimate:p.value,
    names_to = "parameter", 
    values_to = "estimate") |> 
  group_by(parameter, mu_vec) |> 
  summarize(
    emp_mean = mean(estimate),
    emp_var = var(estimate)) |> 
  knitr::kable(digits = 5)
```

    ## `summarise()` has grouped output by 'parameter'. You can override using the
    ## `.groups` argument.

| parameter | mu_vec | emp_mean | emp_var |
|:----------|-------:|---------:|--------:|
| estimate  |      0 |  0.03207 | 0.82778 |
| estimate  |      1 |  0.98736 | 0.83798 |
| estimate  |      2 |  1.96742 | 0.83348 |
| estimate  |      3 |  2.99928 | 0.82707 |
| estimate  |      4 |  3.99906 | 0.84713 |
| estimate  |      5 |  5.02425 | 0.83410 |
| estimate  |      6 |  5.99704 | 0.84558 |
| p.value   |      0 |  0.49937 | 0.08380 |
| p.value   |      1 |  0.34899 | 0.08934 |
| p.value   |      2 |  0.12522 | 0.03921 |
| p.value   |      3 |  0.02429 | 0.00517 |
| p.value   |      4 |  0.00332 | 0.00045 |
| p.value   |      5 |  0.00028 | 0.00000 |
| p.value   |      6 |  0.00003 | 0.00000 |

With the table containing empirical mean and variance for estimate and
p-value, we expect an positive association between the power and $\mu$
because the p-value decreases as $\mu$ increases. This assumption will
be verified in later part.

``` r
power_df <- sim_results_df %>%
  group_by(mu_vec) %>%
  summarize(
    power = mean(p.value < alpha)
  )

ggplot(power_df, aes(x = mu_vec, y = power)) +
  geom_line(color = 'blue') +
  geom_point(color = 'red', size = 2) +
  labs(title = 'Power Curve of One-Sample t-test',
       x = expression('True Mean ' * mu),
       y = 'Power (Probability of Rejecting H0)') +
  theme_minimal()
```

![](p8105_hw5_mw3845_files/figure-gfm/Problem2_power_mu_plot-1.png)<!-- -->

By calculating the proportion of times the null hypothesis was rejected
for each true mean value, weâ€™ve constructed a power curve that
illustrates the association between effect size and power. The positive
association confirms that larger effect sizes lead to higher power,
which aligns with theoretical expectations.

``` r
# Calculate average estimate for all samples
average_estimate_df = sim_results_df %>%
  group_by(mu_vec) %>%
  summarise(
    avg_estimate = mean(estimate)
  )

# Calculate average estimate for samples where null was rejected
average_estimate_rejected_df = sim_results_df %>%
  filter(p.value < alpha) %>%
  group_by(mu_vec) %>%
  summarise(
    avg_estimate_rejected = mean(estimate)
  )

combined_estimate_df = left_join(average_estimate_df, average_estimate_rejected_df, by = "mu_vec") %>% 
  pivot_longer(
    cols = c(avg_estimate, avg_estimate_rejected),
    names_to = "Estimate_Type",
    values_to = "Average_Estimate"
  )

combined_estimate_df$Estimate_Type = recode(
  combined_estimate_df$Estimate_Type,
  "avg_estimate" = "All Samples",
  "avg_estimate_rejected" = "Samples with Rejected H0"
)

# Plot the average estimates
ggplot(combined_estimate_df, aes(x = mu_vec, y = Average_Estimate, color = Estimate_Type)) +
  geom_line() +
  geom_point() +
  labs(
    title = expression('Average Estimate of ' * hat(mu) * ' vs True ' * mu),
    x = expression('True Mean ' * mu),
    y = expression('Average Estimate of ' * hat(mu)),
    color = "Estimate Type"
  ) +
  theme_minimal()
```

![](p8105_hw5_mw3845_files/figure-gfm/Problem2_estAvg_vs_trueValueMu-1.png)<!-- -->
